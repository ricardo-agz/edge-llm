{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e632fbac3864c879e7570143dc1e936"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43fcae1298ef4287af6185b4ebca7d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4be012e7add42919f9195d45f939479"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1638027a43eb4761a1cc44f412ac3350"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Llama-3.2-1B\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m pipe \u001B[38;5;241m=\u001B[39m pipeline(\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# torch_dtype=torch.bfloat16,\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe key to life is\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:272\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(chats, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 272\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1268\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1261\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1262\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1265\u001B[0m         )\n\u001B[1;32m   1266\u001B[0m     )\n\u001B[1;32m   1267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1275\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1273\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1274\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1275\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1276\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1175\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1173\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1174\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1175\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1176\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:370\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    368\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 370\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1829\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1826\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1828\u001B[0m device \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m-> 1829\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_special_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs_has_attention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1831\u001B[0m \u001B[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001B[39;00m\n\u001B[1;32m   1832\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m   1833\u001B[0m     \u001B[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001B[39;00m\n\u001B[1;32m   1834\u001B[0m     \u001B[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1678\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_special_tokens\u001B[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001B[0m\n\u001B[1;32m   1672\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1673\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1674\u001B[0m     )\n\u001B[1;32m   1675\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():  \u001B[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001B[39;00m\n\u001B[1;32m   1676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1677\u001B[0m         eos_token_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1678\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[43misin_mps_friendly\u001B[49m\u001B[43m(\u001B[49m\u001B[43melements\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_elements\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_tensor\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m   1679\u001B[0m     ):\n\u001B[1;32m   1680\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask:\n\u001B[1;32m   1681\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[1;32m   1682\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1683\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1684\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`attention_mask` to obtain reliable results.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1685\u001B[0m             )\n",
      "File \u001B[0;32m~/Documents/Neutrino/Code/Router/Research/o1/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:328\u001B[0m, in \u001B[0;36misin_mps_friendly\u001B[0;34m(elements, test_elements)\u001B[0m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mtile(test_elements\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39meq(test_elements\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mbool()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001B[39;00m\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misin\u001B[49m\u001B[43m(\u001B[49m\u001B[43melements\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_elements\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"The key to life is\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.3663, Varentropy: 6.2080\n",
      "Entropy: 1.1530, Varentropy: 3.6669\n",
      "Entropy: 0.9399, Varentropy: 7.2019\n",
      "Entropy: 1.6958, Varentropy: 2.6731\n",
      "Entropy: 0.2166, Varentropy: 1.5497\n",
      "Entropy: 0.0032, Varentropy: 0.0458\n",
      "Entropy: 0.0034, Varentropy: 0.0514\n",
      "Entropy: 4.6963, Varentropy: 6.4685\n",
      "Entropy: 0.0413, Varentropy: 0.3000\n",
      "Entropy: 0.3627, Varentropy: 1.9567\n",
      "Entropy: 0.4752, Varentropy: 1.2332\n",
      "Entropy: 0.0017, Varentropy: 0.0243\n",
      "Entropy: 0.0025, Varentropy: 0.0372\n",
      "Entropy: 3.7137, Varentropy: 4.9036\n",
      "Entropy: 0.6406, Varentropy: 3.0265\n",
      "Entropy: 0.1926, Varentropy: 1.3964\n",
      "Entropy: 0.0291, Varentropy: 0.2954\n",
      "Entropy: 3.1535, Varentropy: 4.6297\n",
      "Entropy: 0.0167, Varentropy: 0.1914\n",
      "Entropy: 1.0265, Varentropy: 0.3667\n",
      "Entropy: 2.0927, Varentropy: 2.3343\n",
      "Entropy: 2.2296, Varentropy: 10.9174\n",
      "Entropy: 1.0605, Varentropy: 7.5476\n",
      "Entropy: 0.1166, Varentropy: 0.6634\n",
      "Entropy: 2.0237, Varentropy: 7.3195\n",
      "Entropy: 0.1069, Varentropy: 0.8749\n",
      "Entropy: 0.2434, Varentropy: 1.5999\n",
      "Entropy: 0.1350, Varentropy: 0.9930\n",
      "Entropy: 0.0097, Varentropy: 0.1171\n",
      "Entropy: 0.0069, Varentropy: 0.0864\n",
      "Entropy: 0.2308, Varentropy: 2.1398\n",
      "Entropy: 0.0743, Varentropy: 0.4716\n",
      "Entropy: 0.0030, Varentropy: 0.0376\n",
      "Entropy: 0.0016, Varentropy: 0.0219\n",
      "Entropy: 0.0004, Varentropy: 0.0064\n",
      "Entropy: 2.7612, Varentropy: 5.2036\n",
      "Entropy: 0.1477, Varentropy: 0.7325\n",
      "Entropy: 0.0016, Varentropy: 0.0237\n",
      "Entropy: 0.0011, Varentropy: 0.0164\n",
      "Entropy: 0.0003, Varentropy: 0.0053\n",
      "Entropy: 3.0941, Varentropy: 4.4367\n",
      "Entropy: 0.8086, Varentropy: 0.8906\n",
      "\n",
      "Generated Text:\n",
      "The capital of France is: 1) Paris, 2) Strasbourg, 3) Lyon 4) Besancon\n",
      "The capital of England is: 1) London, 2) Aberdeen, 3) Birmingham,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"The capital of France is: \"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "past_key_values = None\n",
    "max_length = 50  # Maximum length of the generated sequence\n",
    "temperature = 0.7\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Generate tokens one by one\n",
    "while generated.shape[1] < max_length:\n",
    "    if past_key_values is None:\n",
    "        # For the first step, provide the full input\n",
    "        outputs = model(input_ids=generated)\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = model(input_ids=generated[:, -1:], past_key_values=past_key_values)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1)\n",
    "    print(f\"Entropy: {entropy.item():.4f}, Varentropy: {varentropy.item():.4f}\")\n",
    "\n",
    "    # Sample the next token\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.2101, Varentropy: 1.2655, Token: ' Paris'\n",
      "Entropy: 1.3736, Varentropy: 0.7257, Token: '<|eot_id|>'\n",
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an intelligent, helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "The capital of France is: Paris<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\"system\", \"You are an intelligent, helpful AI assistant.\"),\n",
    "    (\"user\", \"What is the capital of France?\"),\n",
    "    (\"assistant\", \"The capital of France is:\")  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "past_key_values = None\n",
    "max_new_tokens = 50  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if past_key_values is None:\n",
    "        # For the first step, provide the full input\n",
    "        outputs = model(input_ids=generated)\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = model(input_ids=generated[:, -1:], past_key_values=past_key_values)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1)\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Sample the next token\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0])\n",
    "\n",
    "    # Print the entropy, varentropy, and the generated token together\n",
    "    print(f\"Entropy: {entropy.item():.4f}, Varentropy: {varentropy.item():.4f}, Token: '{decoded_token}'\")\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 4.4438, Varentropy: 27.5838, Injecting '<thinking>' token (high uncertainty), Token: '<thinking>'\n",
      "Entropy: 14.3639, Varentropy: 8.8004, Sampling with increased temperature 0.91 (after thinking), Token: ' importer'\n",
      "Entropy: 9.1457, Varentropy: 34.3509, Sampling with increased temperature 0.91 (after thinking), Token: '_info'\n",
      "Entropy: 4.4108, Varentropy: 25.9069, Sampling with increased temperature 0.91 (after thinking), Token: ' ='\n",
      "Entropy: 3.8362, Varentropy: 14.1959, Sampling with increased temperature 0.91 (after thinking), Token: ' get'\n",
      "Entropy: 0.0452, Varentropy: 0.7342, Default sampling, Token: '_user'\n",
      "Entropy: 0.0115, Varentropy: 0.1962, Default sampling, Token: '_info'\n",
      "Entropy: 0.2473, Varentropy: 2.7016, Sampling with adjusted temperature 0.84 and top_k 40, Token: '(user'\n",
      "Entropy: 0.0102, Varentropy: 0.1279, Default sampling, Token: '_id'\n",
      "Entropy: 0.0858, Varentropy: 0.8344, Default sampling, Token: '='\n",
      "Entropy: 0.0496, Varentropy: 0.6552, Default sampling, Token: '789'\n",
      "Entropy: 0.0042, Varentropy: 0.0655, Greedy sampling (confident), Token: '0'\n",
      "Entropy: 0.7884, Varentropy: 3.6068, Sampling with adjusted temperature 0.84 and top_k 40, Token: ','\n",
      "Entropy: 0.0231, Varentropy: 0.3582, Default sampling, Token: ' special'\n",
      "Entropy: 1.2409, Varentropy: 2.3902, Sampling with adjusted temperature 0.84 and top_k 40, Token: '=\"'\n",
      "Entropy: 0.3080, Varentropy: 2.1692, Sampling with adjusted temperature 0.84 and top_k 40, Token: 'black'\n",
      "Entropy: 0.8144, Varentropy: 3.2356, Sampling with adjusted temperature 0.84 and top_k 40, Token: '\")'\n",
      "Entropy: 1.0722, Varentropy: 1.8850, Sampling with adjusted temperature 0.84 and top_k 40, Token: '<|eot_id|>'\n",
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
      "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
      "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\n",
      "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
      "You SHOULD NOT include any other text in the response.\n",
      "Here is a list of functions in JSON format that you can invoke.[\n",
      "    {\n",
      "        \"name\": \"get_user_info\",\n",
      "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"dict\",\n",
      "            \"required\": [\n",
      "                \"user_id\"\n",
      "            ],\n",
      "            \"properties\": {\n",
      "                \"user_id\": {\n",
      "                \"type\": \"integer\",\n",
      "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
      "            },\n",
      "            \"special\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
      "                \"default\": \"none\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Can you retrieve the details for the user with the ID 7890, who has black as their special request?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<thinking> importer_info = get_user_info(user_id=7890, special=\"black\")<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>\",\n",
    "        \"<|end_header_id|>\",\n",
    "        \"<|eot_id|>\",\n",
    "        \"<thinking>\",\n",
    "        \"</thinking>\",\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert in composing functions. You are given a question and a set of possible functions.\\n\"\n",
    "        \"Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\\n\"\n",
    "        \"If none of the functions can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\\n\"\n",
    "        \"If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\"\n",
    "        \"You SHOULD NOT include any other text in the response.\\n\"\n",
    "        \"Here is a list of functions in JSON format that you can invoke.[\\n\"\n",
    "        '    {\\n        \"name\": \"get_user_info\",\\n        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\\n        \"parameters\": {\\n            \"type\": \"dict\",\\n            \"required\": [\\n                \"user_id\"\\n            ],\\n            \"properties\": {\\n                \"user_id\": {\\n                \"type\": \"integer\",\\n                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\\n            },\\n            \"special\": {\\n                \"type\": \"string\",\\n                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\\n                \"default\": \"none\"\\n                }\\n            }\\n        }\\n    }\\n]',\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"Can you retrieve the details for the user with the ID 7890, who has black as their special request?\",\n",
    "    ),\n",
    "    (\"assistant\", \"\"),  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "past_key_values = None\n",
    "max_new_tokens = 50  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Tokens to check for (e.g., '<thinking>' token)\n",
    "thinking_token = \"<thinking>\"\n",
    "thinking_token_id = tokenizer.convert_tokens_to_ids(thinking_token)\n",
    "end_thinking_token = \"</thinking>\"\n",
    "end_thinking_token_id = tokenizer.convert_tokens_to_ids(end_thinking_token)\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if past_key_values is None:\n",
    "        # For the first step, provide the full input\n",
    "        outputs = model(input_ids=generated)\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = model(input_ids=generated[:, -1:], past_key_values=past_key_values)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Compute probabilities before temperature scaling for entropy calculation\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(\n",
    "        probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1\n",
    "    )\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Adjust sampling strategy based on entropy and varentropy\n",
    "    if entropy.item() < 0.5 and varentropy.item() < 0.1:\n",
    "        # Low entropy and low varentropy: model is confident\n",
    "        # Take argmax (greedy)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        sampling_info = \"Greedy sampling (confident)\"\n",
    "    elif entropy.item() > 3.0 and varentropy.item() > 1.0:\n",
    "        # High entropy and high varentropy: model is very uncertain\n",
    "        # Inject '<thinking>' token\n",
    "        if thinking_token_id not in generated:\n",
    "            # Inject '<thinking>' token\n",
    "            next_token = torch.tensor([[thinking_token_id]], device=device)\n",
    "            sampling_info = \"Injecting '<thinking>' token (high uncertainty)\"\n",
    "        else:\n",
    "            # Continue sampling with adjusted parameters\n",
    "            adjusted_temperature = min(1.5, temperature * 1.3)\n",
    "            next_token_logits = next_token_logits / adjusted_temperature\n",
    "            # Sample the next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sampling_info = f\"Sampling with increased temperature {adjusted_temperature:.2f} (after thinking)\"\n",
    "    elif entropy.item() < 5.0 and varentropy.item() > 1.0:\n",
    "        # Low entropy and high varentropy: model is exploring options\n",
    "        # Adjust temperature and top_k\n",
    "        adjusted_temperature = min(1.5, temperature * 1.2)\n",
    "        top_k = 40  # Adjusted top_k\n",
    "        # Apply temperature scaling\n",
    "        next_token_logits = next_token_logits / adjusted_temperature\n",
    "        # Apply top_k sampling\n",
    "        values, indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
    "        probs = F.softmax(values, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        next_token = indices.gather(-1, next_token)\n",
    "        sampling_info = (\n",
    "            f\"Sampling with adjusted temperature {adjusted_temperature:.2f} and top_k {top_k}\"\n",
    "        )\n",
    "    else:\n",
    "        # Default sampling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = \"Default sampling\"\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0])\n",
    "\n",
    "    # Print the entropy, varentropy, sampling info, and the generated token\n",
    "    print(\n",
    "        f\"Entropy: {entropy.item():.4f}, Varentropy: {varentropy.item():.4f}, {sampling_info}, Token: '{decoded_token}'\"\n",
    "    )\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.5499, Varentropy: 12.1908, Sampling with increased temperature 0.91 (after thinking), Token: ' \n",
      "\n",
      "'\n",
      "Entropy: 4.8438, Varentropy: 9.7476, Sampling with increased temperature 0.91 (after thinking), Token: 'We'\n",
      "Entropy: 2.0792, Varentropy: 4.0139, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' can'\n",
      "Entropy: 4.2960, Varentropy: 7.4074, Sampling with increased temperature 0.91 (after thinking), Token: ' think'\n",
      "Entropy: 0.8434, Varentropy: 3.8189, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' of'\n",
      "Entropy: 2.5318, Varentropy: 5.8520, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' placing'\n",
      "Entropy: 1.9199, Varentropy: 8.3964, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' the'\n",
      "Entropy: 4.0298, Varentropy: 7.2149, Sampling with increased temperature 0.91 (after thinking), Token: ' ''\n",
      "Entropy: 1.6783, Varentropy: 3.7608, Sampling with adjusted temperature 0.84 and top_k 40, Token: 'I'\n",
      "Entropy: 0.5115, Varentropy: 2.0260, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 1.8061, Varentropy: 6.1783, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' as'\n",
      "Entropy: 4.2772, Varentropy: 8.6081, Sampling with increased temperature 0.91 (after thinking), Token: ' a'\n",
      "Entropy: 1.5015, Varentropy: 7.3823, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' single'\n",
      "Entropy: 2.2104, Varentropy: 5.6698, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' unit'\n",
      "Entropy: 2.9963, Varentropy: 7.0247, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' and'\n",
      "Entropy: 3.8163, Varentropy: 9.8384, Sampling with increased temperature 0.91 (after thinking), Token: ' then'\n",
      "Entropy: 4.3046, Varentropy: 10.1522, Sampling with increased temperature 0.91 (after thinking), Token: ' trying'\n",
      "Entropy: 1.0467, Varentropy: 4.4242, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' to'\n",
      "Entropy: 3.2310, Varentropy: 8.8783, Sampling with increased temperature 0.91 (after thinking), Token: ' insert'\n",
      "Entropy: 2.9341, Varentropy: 6.1041, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' the'\n",
      "Entropy: 3.0684, Varentropy: 7.3694, Sampling with increased temperature 0.91 (after thinking), Token: ' ''\n",
      "Entropy: 1.5855, Varentropy: 5.0506, Sampling with adjusted temperature 0.84 and top_k 40, Token: 'S'\n",
      "Entropy: 0.7818, Varentropy: 5.4095, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 3.7794, Varentropy: 6.5138, Sampling with increased temperature 0.91 (after thinking), Token: ' and'\n",
      "Entropy: 1.7167, Varentropy: 6.6262, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' ''\n",
      "Entropy: 1.6748, Varentropy: 2.1584, Sampling with adjusted temperature 0.84 and top_k 40, Token: 'M'\n",
      "Entropy: 0.4028, Varentropy: 2.6960, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 3.8605, Varentropy: 7.5041, Sampling with increased temperature 0.91 (after thinking), Token: ' in'\n",
      "Entropy: 2.8045, Varentropy: 4.6450, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' between'\n",
      "Entropy: 3.7517, Varentropy: 5.5572, Sampling with increased temperature 0.91 (after thinking), Token: ' so'\n",
      "Entropy: 0.8632, Varentropy: 3.8288, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' that'\n",
      "Entropy: 1.9545, Varentropy: 4.9802, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' no'\n",
      "Entropy: 0.7296, Varentropy: 2.6632, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' two'\n",
      "Entropy: 1.1922, Varentropy: 6.1085, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' ''\n",
      "Entropy: 0.0596, Varentropy: 0.5743, Default sampling, Token: 'I'\n",
      "Entropy: 0.0366, Varentropy: 0.3477, Default sampling, Token: ''s'\n",
      "Entropy: 0.3553, Varentropy: 3.0263, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' are'\n",
      "Entropy: 0.4952, Varentropy: 3.0214, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' adjacent'\n",
      "Entropy: 2.3271, Varentropy: 5.4020, Sampling with adjusted temperature 0.84 and top_k 40, Token: '.'\n",
      "Entropy: 4.5394, Varentropy: 6.8217, Sampling with increased temperature 0.91 (after thinking), Token: ' The'\n",
      "Entropy: 4.6662, Varentropy: 11.4851, Sampling with increased temperature 0.91 (after thinking), Token: ' '\n",
      "Entropy: 3.6041, Varentropy: 6.4930, Sampling with increased temperature 0.91 (after thinking), Token: '5'\n",
      "Entropy: 2.7377, Varentropy: 14.2753, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' ''\n",
      "Entropy: 1.3175, Varentropy: 2.0807, Sampling with adjusted temperature 0.84 and top_k 40, Token: 'I'\n",
      "Entropy: 0.0837, Varentropy: 0.6887, Default sampling, Token: ''s'\n",
      "Entropy: 2.8088, Varentropy: 6.5258, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' are'\n",
      "Entropy: 6.6717, Varentropy: 9.8774, Sampling with increased temperature 0.91 (after thinking), Token: ' not'\n",
      "Entropy: 3.9550, Varentropy: 15.5732, Sampling with increased temperature 0.91 (after thinking), Token: ' in'\n",
      "Entropy: 3.0084, Varentropy: 9.2553, Sampling with increased temperature 0.91 (after thinking), Token: ' a'\n",
      "Entropy: 2.7861, Varentropy: 9.1658, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' row'\n",
      "Entropy: 3.1321, Varentropy: 7.7068, Sampling with increased temperature 0.91 (after thinking), Token: '.'\n",
      "Entropy: 4.9356, Varentropy: 7.3904, Sampling with increased temperature 0.91 (after thinking), Token: ' Let'\n",
      "Entropy: 0.6540, Varentropy: 3.8059, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 4.3766, Varentropy: 7.4149, Sampling with increased temperature 0.91 (after thinking), Token: ' start'\n",
      "Entropy: 1.6652, Varentropy: 5.4974, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' with'\n",
      "Entropy: 3.5932, Varentropy: 10.5441, Sampling with increased temperature 0.91 (after thinking), Token: ' the'\n",
      "Entropy: 4.8493, Varentropy: 12.6714, Sampling with increased temperature 0.91 (after thinking), Token: ' number'\n",
      "Entropy: 0.2348, Varentropy: 2.2405, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' of'\n",
      "Entropy: 2.6564, Varentropy: 11.2860, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' ways'\n",
      "Entropy: 1.4605, Varentropy: 4.0048, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' to'\n",
      "Entropy: 2.3402, Varentropy: 4.2442, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' arrange'\n",
      "Entropy: 2.3493, Varentropy: 4.8657, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' the'\n",
      "Entropy: 2.9936, Varentropy: 10.4413, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' '\n",
      "Entropy: 3.3376, Varentropy: 1.7764, Sampling with increased temperature 0.91 (after thinking), Token: '10'\n",
      "Entropy: 1.7119, Varentropy: 8.0246, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' letters'\n",
      "Entropy: 4.8444, Varentropy: 6.8115, Sampling with increased temperature 0.91 (after thinking), Token: ' in'\n",
      "Entropy: 3.7251, Varentropy: 9.6589, Sampling with increased temperature 0.91 (after thinking), Token: ' a'\n",
      "Entropy: 3.5570, Varentropy: 7.9799, Sampling with increased temperature 0.91 (after thinking), Token: ' circular'\n",
      "Entropy: 2.4063, Varentropy: 5.5620, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' manner'\n",
      "Entropy: 4.1513, Varentropy: 7.5368, Sampling with increased temperature 0.91 (after thinking), Token: ').\n",
      "\n",
      "'\n",
      "Entropy: 5.3612, Varentropy: 12.2775, Sampling with increased temperature 0.91 (after thinking), Token: 'First'\n",
      "Entropy: 1.6904, Varentropy: 9.1251, Sampling with adjusted temperature 0.84 and top_k 40, Token: ','\n",
      "Entropy: 2.4579, Varentropy: 8.1415, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' we'\n",
      "Entropy: 3.5899, Varentropy: 6.4624, Sampling with increased temperature 0.91 (after thinking), Token: ''ll'\n",
      "Entropy: 4.5921, Varentropy: 6.4015, Sampling with increased temperature 0.91 (after thinking), Token: ' put'\n",
      "Entropy: 1.5931, Varentropy: 5.0569, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' the'\n",
      "Entropy: 2.2819, Varentropy: 9.1205, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' ''\n",
      "Entropy: 1.8587, Varentropy: 3.0946, Sampling with adjusted temperature 0.84 and top_k 40, Token: 'M'\n",
      "Entropy: 1.2343, Varentropy: 6.8630, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 3.0675, Varentropy: 5.4075, Sampling with increased temperature 0.91 (after thinking), Token: ' together'\n",
      "Entropy: 3.6567, Varentropy: 5.0486, Sampling with increased temperature 0.91 (after thinking), Token: ' ('\n",
      "Entropy: 5.6139, Varentropy: 11.2283, Sampling with increased temperature 0.91 (after thinking), Token: 'there'\n",
      "Entropy: 0.5356, Varentropy: 2.6513, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' are'\n",
      "Entropy: 0.6548, Varentropy: 5.9480, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' '\n",
      "Entropy: 3.0486, Varentropy: 1.5428, Sampling with increased temperature 0.91 (after thinking), Token: '2'\n",
      "Entropy: 4.6702, Varentropy: 10.6008, Sampling with increased temperature 0.91 (after thinking), Token: ' M'\n",
      "Entropy: 0.0748, Varentropy: 1.0661, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 3.5788, Varentropy: 5.6316, Sampling with increased temperature 0.91 (after thinking), Token: ' and'\n",
      "Entropy: 2.1916, Varentropy: 8.0322, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' '\n",
      "Entropy: 2.8645, Varentropy: 1.5943, Sampling with adjusted temperature 0.84 and top_k 40, Token: '2'\n",
      "Entropy: 2.5662, Varentropy: 11.1477, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' S'\n",
      "Entropy: 0.0837, Varentropy: 1.1066, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 4.3689, Varentropy: 6.6873, Sampling with increased temperature 0.91 (after thinking), Token: ' and'\n",
      "Entropy: 1.8494, Varentropy: 9.4363, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' '\n",
      "Entropy: 2.4057, Varentropy: 1.4707, Sampling with adjusted temperature 0.84 and top_k 40, Token: '2'\n",
      "Entropy: 2.6297, Varentropy: 10.5315, Sampling with adjusted temperature 0.84 and top_k 40, Token: ' P'\n",
      "Entropy: 0.1609, Varentropy: 1.9067, Sampling with adjusted temperature 0.84 and top_k 40, Token: ''s'\n",
      "Entropy: 4.5282, Varentropy: 7.0909, Sampling with increased temperature 0.91 (after thinking), Token: ').'\n",
      "Entropy: 4.5162, Varentropy: 7.3387, Sampling with increased temperature 0.91 (after thinking), Token: ' \n",
      "\n",
      "'\n",
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about problems and include your thoughts in a <thinking></thinking> tag before outputting your final response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "How many ways can you arrange the letters in the word 'MISSISSIPPI' such that no two 'I's are adjacent?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<|begin_of_text|>Hmm... let me think... \n",
      "\n",
      "We can think of placing the 'I's as a single unit and then trying to insert the 'S's and 'M's in between so that no two 'I's are adjacent. The 5 'I's are not in a row. Let's start with the number of ways to arrange the 10 letters in a circular manner).\n",
      "\n",
      "First, we'll put the 'M's together (there are 2 M's and 2 S's and 2 P's). \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add special tokens to the tokenizer if needed (e.g., for <|eot_id|>)\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>\",\n",
    "        \"<|end_header_id|>\",\n",
    "        \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\n",
    "        \"system\", \"You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about problems and include your thoughts in a <thinking></thinking> tag before outputting your final response.\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"How many ways can you arrange the letters in the word 'MISSISSIPPI' such that no two 'I's are adjacent?\",\n",
    "    ),\n",
    "    (\"assistant\", \"\"),  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "past_key_values = None\n",
    "max_new_tokens = 100  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Flag to check if thinking phrase has been injected\n",
    "thinking_injected = False\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if past_key_values is None:\n",
    "        # For the first step, provide the full input\n",
    "        outputs = model(input_ids=generated)\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = model(input_ids=generated[:, -1:], past_key_values=past_key_values)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Compute probabilities before temperature scaling for entropy calculation\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(\n",
    "        probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1\n",
    "    )\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Adjust sampling strategy based on entropy and varentropy\n",
    "    if entropy.item() < 0.5 and varentropy.item() < 0.1:\n",
    "        # Low entropy and low varentropy: model is confident\n",
    "        # Take argmax (greedy)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        sampling_info = \"Greedy sampling (confident)\"\n",
    "    elif entropy.item() > 3.0 and varentropy.item() > 1.0:\n",
    "        # High entropy and high varentropy: model is very uncertain\n",
    "        # Inject 'Hmm... let me think...'\n",
    "        if not thinking_injected:\n",
    "            # Inject 'Hmm... let me think...'\n",
    "            thinking_phrase = \"Hmm... let me think...\"\n",
    "            thinking_tokens = tokenizer.encode(thinking_phrase, return_tensors=\"pt\").to(device)\n",
    "            # Append thinking tokens to generated\n",
    "            generated = torch.cat((generated, thinking_tokens[:, :1]), dim=-1)\n",
    "            # Process the first token to update past_key_values\n",
    "            outputs = model(input_ids=thinking_tokens[:, :1], past_key_values=past_key_values)\n",
    "            past_key_values = outputs.past_key_values\n",
    "            # Loop over the rest of the thinking tokens\n",
    "            for idx in range(1, thinking_tokens.size(1)):\n",
    "                next_token = thinking_tokens[:, idx:idx+1]\n",
    "                generated = torch.cat((generated, next_token), dim=-1)\n",
    "                outputs = model(input_ids=next_token, past_key_values=past_key_values)\n",
    "                past_key_values = outputs.past_key_values\n",
    "            sampling_info = \"Injecting 'Hmm... let me think...' (high uncertainty)\"\n",
    "            thinking_injected = True\n",
    "            continue  # Proceed to next iteration\n",
    "        else:\n",
    "            # Continue sampling with adjusted parameters\n",
    "            adjusted_temperature = min(1.5, temperature * 1.3)\n",
    "            next_token_logits = next_token_logits / adjusted_temperature\n",
    "            # Sample the next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sampling_info = f\"Sampling with increased temperature {adjusted_temperature:.2f} (after thinking)\"\n",
    "    elif entropy.item() < 5.0 and varentropy.item() > 1.0:\n",
    "        # Low entropy and high varentropy: model is exploring options\n",
    "        # Adjust temperature and top_k\n",
    "        adjusted_temperature = min(1.5, temperature * 1.2)\n",
    "        top_k = 40  # Adjusted top_k\n",
    "        # Apply temperature scaling\n",
    "        next_token_logits = next_token_logits / adjusted_temperature\n",
    "        # Apply top_k sampling\n",
    "        values, indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
    "        probs = F.softmax(values, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        next_token = indices.gather(-1, next_token)\n",
    "        sampling_info = (\n",
    "            f\"Sampling with adjusted temperature {adjusted_temperature:.2f} and top_k {top_k}\"\n",
    "        )\n",
    "    else:\n",
    "        # Default sampling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = \"Default sampling\"\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0])\n",
    "\n",
    "    # Print the entropy, varentropy, sampling info, and the generated token\n",
    "    print(\n",
    "        f\"Entropy: {entropy.item():.4f}, Varentropy: {varentropy.item():.4f}, {sampling_info}, Token: '{decoded_token}'\"\n",
    "    )\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd69ed1bb1b243f7a4292f9a61e57f50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4dac57bfd6b44fa8d3928ed1c7b3633"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "deb8de82641e4ec5a2a5389cc1b54cb1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ca80b88de304febbc48d26b76494726"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cf2ed1de31246908e964210f7873038"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa61bf0e0fe6455f82218f79ce134fb1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ecc864898804504879cdaa22ae40a19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to large model (uncertain).\n",
      "Entropy: 2.2021, Varentropy: 6.6188, Default sampling, Token: 'To'\n",
      "Entropy: 1.6461, Varentropy: 4.6722, Default sampling, Token: ' solve'\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0041, Varentropy: 0.0492, Greedy sampling (confident), Token: ' this'\n",
      "Entropy: 1.9102, Varentropy: 11.9209, Default sampling, Token: ' problem'\n",
      "Entropy: 0.3162, Varentropy: 2.8725, Default sampling, Token: ','\n",
      "Entropy: 1.4102, Varentropy: 3.1123, Default sampling, Token: ' we'\n",
      "Entropy: 1.4473, Varentropy: 3.5699, Default sampling, Token: ' can'\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 3.4094, Varentropy: 10.7237, Sampling with increased temperature 0.91 (after thinking), Token: ' use'\n",
      "Entropy: 0.7832, Varentropy: 1.9896, Default sampling, Token: ' the'\n",
      "Entropy: 0.9345, Varentropy: 5.7506, Default sampling, Token: ' concept'\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0002, Varentropy: 0.0041, Greedy sampling (confident), Token: ' of'\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 10.6968, Varentropy: 17.7701, Sampling with increased temperature 0.91 (after thinking), Token: ' permutations'\n",
      "Entropy: 0.8922, Varentropy: 1.8064, Default sampling, Token: ' with'\n",
      "Entropy: 1.8114, Varentropy: 4.0483, Default sampling, Token: ' restrictions'\n",
      "Entropy: 1.0092, Varentropy: 1.5860, Default sampling, Token: '.'\n",
      "Entropy: 1.8692, Varentropy: 4.1039, Default sampling, Token: ' We'\n",
      "Entropy: 1.6813, Varentropy: 2.9735, Default sampling, Token: ' have'\n",
      "Entropy: 1.1451, Varentropy: 1.5029, Default sampling, Token: ' a'\n",
      "Entropy: 0.8227, Varentropy: 2.4119, Default sampling, Token: ' total'\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0024, Varentropy: 0.0298, Greedy sampling (confident), Token: ' of'\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 3.0761, Varentropy: 10.0099, Sampling with increased temperature 0.91 (after thinking), Token: ' '\n",
      "Entropy: 1.6547, Varentropy: 0.9915, Default sampling, Token: '11'\n",
      "Entropy: 0.0257, Varentropy: 0.2741, Default sampling, Token: ' letters'\n",
      "Entropy: 0.5511, Varentropy: 1.8728, Default sampling, Token: ' in'\n",
      "Entropy: 0.0649, Varentropy: 0.4481, Default sampling, Token: ' the'\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0004, Varentropy: 0.0079, Greedy sampling (confident), Token: ' word'\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 9.1178, Varentropy: 24.9051, Sampling with increased temperature 0.91 (after thinking), Token: ' ''\n",
      "Entropy: 0.0334, Varentropy: 0.3775, Default sampling, Token: 'MISS'\n",
      "Entropy: 0.0117, Varentropy: 0.1523, Default sampling, Token: 'ISS'\n",
      "Entropy: 0.0086, Varentropy: 0.1054, Default sampling, Token: 'IP'\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0006, Varentropy: 0.0100, Greedy sampling (confident), Token: '''\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 11.8820, Varentropy: 14.3637, Sampling with increased temperature 0.91 (after thinking), Token: ' -'\n",
      "Entropy: 0.9521, Varentropy: 3.0218, Default sampling, Token: ' '\n",
      "Entropy: 1.6134, Varentropy: 3.8639, Default sampling, Token: '4'\n",
      "Entropy: 1.7554, Varentropy: 2.6134, Default sampling, Token: ' ''\n",
      "Entropy: 0.9179, Varentropy: 0.7304, Default sampling, Token: 'S'\n",
      "Entropy: 0.6138, Varentropy: 1.5771, Default sampling, Token: ''s'\n",
      "Entropy: 0.0407, Varentropy: 0.3464, Default sampling, Token: ','\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0076, Varentropy: 0.0881, Greedy sampling (confident), Token: ' '\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 4.9610, Varentropy: 8.2916, Sampling with increased temperature 0.91 (after thinking), Token: '4'\n",
      "Switching to small model (confident).\n",
      "Entropy: 0.0030, Varentropy: 0.0526, Greedy sampling (confident), Token: 'Title'\n",
      "Switching to large model (uncertain).\n",
      "Entropy: 12.1527, Varentropy: 17.4081, Sampling with increased temperature 0.91 (after thinking), Token: ':'\n",
      "Entropy: 9.5093, Varentropy: 27.4283, Sampling with increased temperature 0.91 (after thinking), Token: ' The'\n",
      "Entropy: 4.4001, Varentropy: 21.9977, Sampling with increased temperature 0.91 (after thinking), Token: ' problem'\n",
      "Entropy: 3.5279, Varentropy: 6.1130, Sampling with increased temperature 0.91 (after thinking), Token: ' is'\n",
      "Entropy: 2.1742, Varentropy: 7.7314, Default sampling, Token: ' to'\n",
      "Entropy: 1.3849, Varentropy: 1.7334, Default sampling, Token: ' arrange'\n",
      "Entropy: 1.2444, Varentropy: 1.4407, Default sampling, Token: ' these'\n",
      "Entropy: 0.5837, Varentropy: 1.6087, Default sampling, Token: ' letters'\n",
      "Entropy: 1.0421, Varentropy: 1.7140, Default sampling, Token: ' in'\n",
      "Entropy: 0.6876, Varentropy: 1.9358, Default sampling, Token: ' such'\n",
      "Entropy: 0.0602, Varentropy: 0.4381, Default sampling, Token: ' a'\n",
      "Entropy: 0.0197, Varentropy: 0.1892, Default sampling, Token: ' way'\n",
      "Entropy: 0.0313, Varentropy: 0.5161, Default sampling, Token: ' that'\n",
      "Entropy: 0.0485, Varentropy: 0.5443, Default sampling, Token: ' no'\n",
      "Entropy: 0.0238, Varentropy: 0.2831, Default sampling, Token: ' two'\n",
      "Entropy: 0.1145, Varentropy: 1.1528, Default sampling, Token: ' ''\n",
      "Entropy: 0.0650, Varentropy: 0.7195, Default sampling, Token: 'I'\n",
      "Entropy: 0.0267, Varentropy: 0.2687, Default sampling, Token: ''s'\n",
      "Entropy: 0.0694, Varentropy: 0.8335, Default sampling, Token: ' are'\n",
      "Entropy: 0.1239, Varentropy: 1.0307, Default sampling, Token: ' adjacent'\n",
      "Entropy: 2.0598, Varentropy: 2.3339, Default sampling, Token: '.\n",
      "\n",
      "'\n",
      "Entropy: 3.1462, Varentropy: 7.3756, Sampling with increased temperature 0.91 (after thinking), Token: ' Thinking'\n",
      "Entropy: 2.7496, Varentropy: 11.0883, Default sampling, Token: ':'\n",
      "Entropy: 4.4389, Varentropy: 6.3686, Sampling with increased temperature 0.91 (after thinking), Token: ' It'\n",
      "Entropy: 2.2543, Varentropy: 2.6784, Default sampling, Token: ' might'\n",
      "Entropy: 0.2953, Varentropy: 1.4797, Default sampling, Token: ' be'\n",
      "Entropy: 1.3772, Varentropy: 4.0562, Default sampling, Token: ' easier'\n",
      "Entropy: 0.0503, Varentropy: 0.3836, Default sampling, Token: ' to'\n",
      "Entropy: 2.9846, Varentropy: 2.8562, Default sampling, Token: ' first'\n",
      "Entropy: 2.7388, Varentropy: 2.9641, Default sampling, Token: ' arrange'\n",
      "Entropy: 0.0661, Varentropy: 0.5225, Default sampling, Token: ' the'\n",
      "Entropy: 2.1906, Varentropy: 2.9644, Default sampling, Token: ' non'\n",
      "Entropy: 0.9970, Varentropy: 2.6182, Default sampling, Token: '-''\n",
      "Entropy: 0.2277, Varentropy: 1.2073, Default sampling, Token: 'I'\n",
      "Entropy: 0.9279, Varentropy: 0.5167, Default sampling, Token: '''\n",
      "Entropy: 0.2411, Varentropy: 1.6711, Default sampling, Token: ' letters'\n",
      "Entropy: 2.3202, Varentropy: 3.1106, Default sampling, Token: ','\n",
      "Entropy: 2.4119, Varentropy: 4.1897, Default sampling, Token: ' and'\n",
      "Entropy: 0.0286, Varentropy: 0.4155, Default sampling, Token: ' then'\n",
      "Entropy: 1.3435, Varentropy: 5.1067, Default sampling, Token: ' insert'\n",
      "Entropy: 0.1058, Varentropy: 0.7515, Default sampling, Token: ' the'\n",
      "Entropy: 0.1967, Varentropy: 1.4820, Default sampling, Token: ' ''\n",
      "Entropy: 0.0115, Varentropy: 0.1614, Default sampling, Token: 'I'\n",
      "Entropy: 0.0802, Varentropy: 0.5115, Default sampling, Token: ''s'\n",
      "Entropy: 1.2567, Varentropy: 4.8982, Default sampling, Token: ' in'\n",
      "Entropy: 0.7968, Varentropy: 1.4318, Default sampling, Token: ' the'\n",
      "Entropy: 1.6806, Varentropy: 2.8410, Default sampling, Token: ' gaps'\n",
      "Entropy: 1.4225, Varentropy: 4.6338, Default sampling, Token: '.'\n",
      "Entropy: 2.7832, Varentropy: 4.9647, Default sampling, Token: ' We'\n",
      "Entropy: 0.3732, Varentropy: 1.8586, Default sampling, Token: ' have'\n",
      "Entropy: 0.1267, Varentropy: 1.2787, Default sampling, Token: ' '\n",
      "Entropy: 0.7402, Varentropy: 3.0577, Default sampling, Token: '7'\n",
      "Entropy: 0.5886, Varentropy: 3.2054, Default sampling, Token: ' non'\n",
      "Entropy: 0.0215, Varentropy: 0.2234, Default sampling, Token: '-''\n",
      "Entropy: 0.0140, Varentropy: 0.1585, Default sampling, Token: 'I'\n",
      "Entropy: 0.1126, Varentropy: 0.6079, Default sampling, Token: '''\n",
      "Entropy: 0.0225, Varentropy: 0.2413, Default sampling, Token: ' letters'\n",
      "Entropy: 2.5485, Varentropy: 1.7921, Default sampling, Token: ':'\n",
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about problems and include your thoughts in a <thinking></thinking> tag before outputting your final response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "How many ways can you arrange the letters in the word 'MISSISSIPPI' such that no two 'I's are adjacent?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<|begin_of_text|><thinking>Hmm... let me think...</thinking>\n",
      "To solve this problem, we can use the concept of permutations with restrictions. We have a total of 11 letters in the word 'MISSISSIP' - 4 'S's, 4Title: The problem is to arrange these letters in such a way that no two 'I's are adjacent.\n",
      "\n",
      " Thinking: It might be easier to first arrange the non-'I' letters, and then insert the 'I's in the gaps. We have 7 non-'I' letters:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Add special tokens to the tokenizer if needed\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>\",\n",
    "        \"<|end_header_id|>\",\n",
    "        \"<|eot_id|>\",\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load both models\n",
    "model_small_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_large_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model_small = AutoModelForCausalLM.from_pretrained(\n",
    "    model_small_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model_large = AutoModelForCausalLM.from_pretrained(\n",
    "    model_large_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Resize token embeddings if special tokens were added\n",
    "model_small.resize_token_embeddings(len(tokenizer))\n",
    "model_large.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model_small.eval()\n",
    "model_large.eval()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_small.to(device)\n",
    "model_large.to(device)\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "                f\"{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"user\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"assistant\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about problems and include your thoughts in a <thinking></thinking> tag before outputting your final response.\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"How many ways can you arrange the letters in the word 'MISSISSIPPI' such that no two 'I's are adjacent?\",\n",
    "    ),\n",
    "    (\"assistant\", \"\"),  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "max_new_tokens = 100  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Flags and variables to manage model switching\n",
    "thinking_injected = False\n",
    "current_model = model_small\n",
    "current_past_key_values = None\n",
    "tokens_since_last_switch = 0  # Number of tokens generated since last model switch\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if current_past_key_values is None:\n",
    "        # For the first step or after switching models, provide the full input\n",
    "        outputs = current_model(input_ids=generated)\n",
    "        tokens_since_last_switch = 0\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = current_model(\n",
    "            input_ids=generated[:, -1:], past_key_values=current_past_key_values\n",
    "        )\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    current_past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Compute probabilities before temperature scaling for entropy calculation\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(\n",
    "        probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1\n",
    "    )\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Adjust sampling strategy based on entropy and varentropy\n",
    "    if entropy.item() < 0.5 and varentropy.item() < 0.1:\n",
    "        # Low entropy and low varentropy: model is confident\n",
    "        # Use the smaller model if not already using it\n",
    "        if current_model != model_small:\n",
    "            print(\"Switching to small model (confident).\")\n",
    "            current_model = model_small\n",
    "            # Recompute past_key_values for the small model\n",
    "            # We need to process the generated tokens since last switch\n",
    "            generated_tokens_to_reprocess = generated[:, -tokens_since_last_switch:]\n",
    "            outputs = current_model(input_ids=generated_tokens_to_reprocess)\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Take argmax (greedy)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        sampling_info = \"Greedy sampling (confident)\"\n",
    "    elif entropy.item() > 3.0 and varentropy.item() > 1.0:\n",
    "        # High entropy and high varentropy: model is very uncertain\n",
    "        # Use the larger model if not already using it\n",
    "        if current_model != model_large:\n",
    "            print(\"Switching to large model (uncertain).\")\n",
    "            current_model = model_large\n",
    "            # Recompute past_key_values for the large model\n",
    "            # We need to process all generated tokens\n",
    "            outputs = current_model(input_ids=generated)\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Inject 'Hmm... let me think...'\n",
    "        if not thinking_injected:\n",
    "            thinking_phrase = \"<thinking>Hmm... let me think...</thinking>\\n\"\n",
    "            thinking_tokens = tokenizer.encode(\n",
    "                thinking_phrase, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            # Append thinking tokens to generated\n",
    "            generated = torch.cat((generated, thinking_tokens[:, :1]), dim=-1)\n",
    "            # Process the first token to update past_key_values\n",
    "            outputs = current_model(\n",
    "                input_ids=thinking_tokens[:, :1], past_key_values=current_past_key_values\n",
    "            )\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            # Loop over the rest of the thinking tokens\n",
    "            for idx in range(1, thinking_tokens.size(1)):\n",
    "                next_token = thinking_tokens[:, idx : idx + 1]\n",
    "                generated = torch.cat((generated, next_token), dim=-1)\n",
    "                outputs = current_model(\n",
    "                    input_ids=next_token, past_key_values=current_past_key_values\n",
    "                )\n",
    "                current_past_key_values = outputs.past_key_values\n",
    "            sampling_info = \"Injecting 'Hmm... let me think...' (high uncertainty)\"\n",
    "            thinking_injected = True\n",
    "            tokens_since_last_switch += thinking_tokens.size(1)\n",
    "            continue  # Proceed to next iteration\n",
    "        else:\n",
    "            # Continue sampling with adjusted parameters\n",
    "            adjusted_temperature = min(1.5, temperature * 1.3)\n",
    "            next_token_logits = next_token_logits / adjusted_temperature\n",
    "            # Sample the next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sampling_info = f\"Sampling with increased temperature {adjusted_temperature:.2f} (after thinking)\"\n",
    "    else:\n",
    "        # Default sampling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = \"Default sampling\"\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "    tokens_since_last_switch += 1\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0])\n",
    "\n",
    "    # Print the entropy, varentropy, sampling info, and the generated token\n",
    "    print(\n",
    "        f\"Entropy: {entropy.item():.4f}, Varentropy: {varentropy.item():.4f}, {sampling_info}, Token: '{decoded_token}'\"\n",
    "    )\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color: blue\">A</span><span style=\"color: red\"> hail</span><span style=\"color: red\"> mary</span><span style=\"color: red\"> in</span><span style=\"color: red\"> the</span><span style=\"color: red\"> NFL</span><span style=\"color: red\"> can</span><span style=\"color: red\"> be</span><span style=\"color: red\"> a</span><span style=\"color: red\"> thrilling</span><span style=\"color: red\"> play</span><span style=\"color: red\"> to</span><span style=\"color: red\"> watch</span><span style=\"color: red\"> and</span><span style=\"color: red\"> defend</span><span style=\"color: red\"> against</span><span style=\"color: red\">,</span><span style=\"color: red\"> but</span><span style=\"color: red\"> ultimately</span><span style=\"color: red\">,</span><span style=\"color: red\"> its</span><span style=\"color: red\"> unpredict</span><span style=\"color: blue\">ability</span><span style=\"color: blue\"> makes</span><span style=\"color: blue\"> it</span><span style=\"color: red\"> challenging</span><span style=\"color: red\">.</span><span style=\"color: red\"> However</span><span style=\"color: blue\">,</span><span style=\"color: red\"> here</span><span style=\"color: red\">'s</span><span style=\"color: blue\"> a</span><span style=\"color: red\"> general</span><span style=\"color: red\"> defensive</span><span style=\"color: red\"> strategy</span><span style=\"color: red\"> to</span><span style=\"color: red\"> consider</span><span style=\"color: red\">:\n\n</span><span style=\"color: red\">**</span><span style=\"color: red\">Basic</span><span style=\"color: red\"> Principles</span><span style=\"color: red\">:</span><span style=\"color: red\">**\n\n</span><span style=\"color: red\">1</span><span style=\"color: blue\">.</span><span style=\"color: blue\"> **</span><span style=\"color: red\">Def</span><span style=\"color: red\">ensive</span><span style=\"color: red\"> alignment</span><span style=\"color: red\">**:</span><span style=\"color: red\"> Line</span><span style=\"color: red\"> up</span><span style=\"color: red\"> in</span><span style=\"color: red\"> a</span><span style=\"color: red\"> nearly</span><span style=\"color: red\"> shell</span><span style=\"color: red\"> or</span><span style=\"color: red\"> Cover</span><span style=\"color: red\"> </span><span style=\"color: red\">2</span><span style=\"color: red\"> deep</span><span style=\"color: red\"> safety</span><span style=\"color: red\"> look</span><span style=\"color: red\">,</span><span style=\"color: red\"> with</span><span style=\"color: red\"> one</span><span style=\"color: red\"> safety</span><span style=\"color: red\"> deep</span><span style=\"color: red\"> (</span><span style=\"color: red\">e</span><span style=\"color: red\">.g</span><span style=\"color: blue\">.,</span><span style=\"color: red\"> a</span><span style=\"color: red\"> single</span><span style=\"color: red\">-high</span><span style=\"color: red\"> safety</span><span style=\"color: red\">).</span><span style=\"color: red\"> This</span><span style=\"color: red\"> alignment</span><span style=\"color: red\"> encourages</span><span style=\"color: red\"> the</span><span style=\"color: red\"> defense</span><span style=\"color: blue\"> of</span><span style=\"color: red\"> the</span><span style=\"color: red\"> ball</span><span style=\"color: red\"> and</span><span style=\"color: red\"> limits</span><span style=\"color: red\"> the</span><span style=\"color: red\"> quarterback</span><span style=\"color: blue\">'s</span><span style=\"color: red\"> options</span><span style=\"color: red\">.\n</span><span style=\"color: blue\">*</span><span style=\"color: red\"> slot</span><span style=\"color: red\"> corner</span><span style=\"color: red\">(s</span><span style=\"color: red\">)</span><span style=\"color: red\"> should</span><span style=\"color: red\"> be</span><span style=\"color: red\"> assigned</span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about the question before outputting your final response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Tell me a good defensive coverage against a hailmary in NFL<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "A hail mary in the NFL can be a thrilling play to watch and defend against, but ultimately, its unpredictability makes it challenging. However, here's a general defensive strategy to consider:\n",
      "\n",
      "**Basic Principles:**\n",
      "\n",
      "1. **Defensive alignment**: Line up in a nearly shell or Cover 2 deep safety look, with one safety deep (e.g., a single-high safety). This alignment encourages the defense of the ball and limits the quarterback's options.\n",
      "* slot corner(s) should be assigned\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# For colored output in Jupyter\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# Define colors for small and large models\n",
    "SMALL_MODEL_COLOR = 'blue'\n",
    "LARGE_MODEL_COLOR = 'red'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Add special tokens to the tokenizer if needed\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>\",\n",
    "        \"<|end_header_id|>\",\n",
    "        \"<|eot_id|>\",\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load both models\n",
    "model_small_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_large_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model_small = AutoModelForCausalLM.from_pretrained(\n",
    "    model_small_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model_large = AutoModelForCausalLM.from_pretrained(\n",
    "    model_large_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Resize token embeddings if special tokens were added\n",
    "model_small.resize_token_embeddings(len(tokenizer))\n",
    "model_large.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model_small.eval()\n",
    "model_large.eval()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_small.to(device)\n",
    "model_large.to(device)\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "                f\"{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"user\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"assistant\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about the question before outputting your final response.\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"Tell me a good defensive coverage against a hailmary in NFL\",\n",
    "    ),\n",
    "    (\"assistant\", \"\"),  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "max_new_tokens = 100  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Flags and variables to manage model switching\n",
    "current_model = model_small\n",
    "current_past_key_values = None\n",
    "tokens_since_last_switch = 0  # Number of tokens generated since last model switch\n",
    "\n",
    "# For storing generated tokens and their colors\n",
    "generated_tokens = []\n",
    "generated_colors = []\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if current_past_key_values is None:\n",
    "        # For the first step or after switching models, provide the full input\n",
    "        outputs = current_model(input_ids=generated)\n",
    "        tokens_since_last_switch = 0\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = current_model(\n",
    "            input_ids=generated[:, -1:], past_key_values=current_past_key_values\n",
    "        )\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    current_past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Compute probabilities before temperature scaling for entropy calculation\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(\n",
    "        probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1\n",
    "    )\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Adjust sampling strategy based on entropy and varentropy\n",
    "    if entropy.item() < 0.75 and varentropy.item() < 0.3:\n",
    "        # Low entropy and low varentropy: model is confident\n",
    "        # Use the smaller model if not already using it\n",
    "        if current_model != model_small:\n",
    "            print(\"\\nSwitching to small model (confident).\")\n",
    "            current_model = model_small\n",
    "            # Recompute past_key_values for the small model\n",
    "            # We need to process the generated tokens since last switch\n",
    "            generated_tokens_to_reprocess = generated[:, -tokens_since_last_switch:]\n",
    "            outputs = current_model(input_ids=generated_tokens_to_reprocess)\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Take argmax (greedy)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        sampling_info = \"Greedy sampling (confident)\"\n",
    "    elif entropy.item() > 4.0 and varentropy.item() > 1.5:\n",
    "        # High entropy and high varentropy: model is very uncertain\n",
    "        # Use the larger model if not already using it\n",
    "        if current_model != model_large:\n",
    "            print(\"\\nSwitching to large model (uncertain).\")\n",
    "            current_model = model_large\n",
    "            # Recompute past_key_values for the large model\n",
    "            # We need to process all generated tokens\n",
    "            outputs = current_model(input_ids=generated)\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Continue sampling with adjusted parameters\n",
    "        adjusted_temperature = min(1.5, temperature * 1.3)\n",
    "        next_token_logits = next_token_logits / adjusted_temperature\n",
    "        # Sample the next token\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = f\"Sampling with increased temperature {adjusted_temperature:.2f} (uncertain)\"\n",
    "    else:\n",
    "        # Default sampling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = \"Default sampling\"\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "    tokens_since_last_switch += 1\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0], skip_special_tokens=False)\n",
    "\n",
    "    # Store the token and its associated color\n",
    "    generated_tokens.append(decoded_token)\n",
    "    if current_model == model_small:\n",
    "        generated_colors.append(SMALL_MODEL_COLOR)\n",
    "    else:\n",
    "        generated_colors.append(LARGE_MODEL_COLOR)\n",
    "\n",
    "    # Display the generated text with colors\n",
    "    # Build the HTML content\n",
    "    html_content = ''\n",
    "    for token, color in zip(generated_tokens, generated_colors):\n",
    "        html_content += f'<span style=\"color: {color}\">{token}</span>'\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "# After generation, print the final generated text\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e105ff64caf84a778ce6515c8f36a791"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c03c570b2904f94b49090c586a4bd1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching to large model (uncertain).\n",
      "\n",
      "Switching to small model (confident).\n",
      "\n",
      "Switching to large model (uncertain).\n",
      "\n",
      "Switching to small model (confident).\n",
      "\n",
      "Switching to large model (uncertain).\n",
      "\n",
      "Switching to small model (confident).\n",
      "\n",
      "Switching to large model (uncertain).\n",
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about problems and include your thoughts in a <thinking></thinking> tag before outputting your final response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Tell me a good defensive coverage against a Hail Mary in the NFL.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "</thinking>\n",
      "\n",
      "To defend against a H1N1 (Hailong or Hail Mary), the key is to identify the possible rookie quarterback and assign an extra defender to be the last line of defense. Here are some strategies that can be employed to defend against a Hail Mary:\n",
      "\n",
      "1. Safe bet: Assign the middle linebacker to the threat leading edge. If the quarterback has a strong arm, have a defensive back who is quicker and can take advantage of the space. Assign safety to provide\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# For colored output in Jupyter\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import Output\n",
    "import time\n",
    "\n",
    "# Define colors for small and large models\n",
    "SMALL_MODEL_COLOR = 'blue'\n",
    "LARGE_MODEL_COLOR = 'red'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Add special tokens to the tokenizer if needed\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>\",\n",
    "        \"<|end_header_id|>\",\n",
    "        \"<|eot_id|>\",\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load both models\n",
    "model_small_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_large_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model_small = AutoModelForCausalLM.from_pretrained(\n",
    "    model_small_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model_large = AutoModelForCausalLM.from_pretrained(\n",
    "    model_large_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Resize token embeddings if special tokens were added\n",
    "model_small.resize_token_embeddings(len(tokenizer))\n",
    "model_large.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model_small.eval()\n",
    "model_large.eval()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_small.to(device)\n",
    "model_large.to(device)\n",
    "\n",
    "# Define presets for model switching\n",
    "presets = {\n",
    "    'conservative': {\n",
    "        'small_model_entropy_threshold': 1.0,\n",
    "        'small_model_varentropy_threshold': 0.2,\n",
    "        'large_model_entropy_threshold': 2.5,\n",
    "        'large_model_varentropy_threshold': 0.8,\n",
    "    },\n",
    "    'balanced': {\n",
    "        'small_model_entropy_threshold': 0.5,\n",
    "        'small_model_varentropy_threshold': 0.1,\n",
    "        'large_model_entropy_threshold': 3.0,\n",
    "        'large_model_varentropy_threshold': 1.0,\n",
    "    },\n",
    "    'aggressive': {\n",
    "        'small_model_entropy_threshold': 0.3,\n",
    "        'small_model_varentropy_threshold': 0.05,\n",
    "        'large_model_entropy_threshold': 4.0,\n",
    "        'large_model_varentropy_threshold': 1.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select a preset\n",
    "selected_preset = 'balanced'  # Change to 'conservative' or 'aggressive' as needed\n",
    "preset = presets[selected_preset]\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "                f\"{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"user\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"assistant\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about problems and include your thoughts in a <thinking></thinking> tag before outputting your final response.\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"Tell me a good defensive coverage against a Hail Mary in the NFL.\",\n",
    "    ),\n",
    "    (\"assistant\", \"\"),  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "max_new_tokens = 100  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Flags and variables to manage model switching\n",
    "current_model = model_small\n",
    "current_past_key_values = None\n",
    "tokens_since_last_switch = 0  # Number of tokens generated since last model switch\n",
    "\n",
    "# For storing generated tokens and their colors\n",
    "generated_tokens = []\n",
    "generated_colors = []\n",
    "\n",
    "# Initialize the Output widget\n",
    "out = Output()\n",
    "display(out)\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if current_past_key_values is None:\n",
    "        # For the first step or after switching models, provide the full input\n",
    "        outputs = current_model(input_ids=generated)\n",
    "        tokens_since_last_switch = 0\n",
    "    else:\n",
    "        # For subsequent steps, use only the last token and past key values\n",
    "        outputs = current_model(\n",
    "            input_ids=generated[:, -1:], past_key_values=current_past_key_values\n",
    "        )\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    current_past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Compute probabilities before temperature scaling for entropy calculation\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(\n",
    "        probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1\n",
    "    )\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Adjust sampling strategy based on entropy and varentropy\n",
    "    if entropy.item() < preset['small_model_entropy_threshold'] and varentropy.item() < preset['small_model_varentropy_threshold']:\n",
    "        # Low entropy and low varentropy: model is confident\n",
    "        # Use the smaller model if not already using it\n",
    "        if current_model != model_small:\n",
    "            print(\"\\nSwitching to small model (confident).\")\n",
    "            current_model = model_small\n",
    "            # Recompute past_key_values for the small model\n",
    "            # We need to process the generated tokens since last switch\n",
    "            generated_tokens_to_reprocess = generated[:, -tokens_since_last_switch:]\n",
    "            outputs = current_model(input_ids=generated_tokens_to_reprocess)\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Take argmax (greedy)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        sampling_info = \"Greedy sampling (confident)\"\n",
    "    elif entropy.item() > preset['large_model_entropy_threshold'] and varentropy.item() > preset['large_model_varentropy_threshold']:\n",
    "        # High entropy and high varentropy: model is very uncertain\n",
    "        # Use the larger model if not already using it\n",
    "        if current_model != model_large:\n",
    "            print(\"\\nSwitching to large model (uncertain).\")\n",
    "            current_model = model_large\n",
    "            # Recompute past_key_values for the large model\n",
    "            # We need to process all generated tokens\n",
    "            outputs = current_model(input_ids=generated)\n",
    "            current_past_key_values = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Continue sampling with adjusted parameters\n",
    "        adjusted_temperature = min(1.5, temperature * 1.3)\n",
    "        next_token_logits = next_token_logits / adjusted_temperature\n",
    "        # Sample the next token\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = f\"Sampling with increased temperature {adjusted_temperature:.2f} (uncertain)\"\n",
    "    else:\n",
    "        # Default sampling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = \"Default sampling\"\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "    tokens_since_last_switch += 1\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0], skip_special_tokens=False)\n",
    "\n",
    "    # Store the token and its associated color\n",
    "    generated_tokens.append(decoded_token)\n",
    "    if current_model == model_small:\n",
    "        generated_colors.append(SMALL_MODEL_COLOR)\n",
    "    else:\n",
    "        generated_colors.append(LARGE_MODEL_COLOR)\n",
    "\n",
    "    # Display the generated text with colors\n",
    "    # Build the HTML content\n",
    "    html_content = ''\n",
    "    for token, color in zip(generated_tokens, generated_colors):\n",
    "        html_content += f'<span style=\"color: {color}\">{token}</span>'\n",
    "\n",
    "    # Update the output\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(html_content))\n",
    "\n",
    "    # Optional sleep to reduce choppiness\n",
    "    time.sleep(0.05)\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "# After generation, print the final generated text\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
