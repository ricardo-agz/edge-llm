{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color: blue\">There</span><span style=\"color: blue\"> are</span><span style=\"color: blue\"> </span><span style=\"color: blue\">2</span><span style=\"color: blue\"> '</span><span style=\"color: blue\">r</span><span style=\"color: blue\">'s</span><span style=\"color: blue\"> in</span><span style=\"color: blue\"> the</span><span style=\"color: blue\"> word</span><span style=\"color: blue\"> \"</span><span style=\"color: blue\">str</span><span style=\"color: blue\">aw</span><span style=\"color: blue\">berry</span><span style=\"color: blue\">\".</span><span style=\"color: blue\"><|eot_id|></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about the question before outputting your final response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "How many 'r's are there in the word strawberry?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "There are 2 'r's in the word \"strawberry\".<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# For colored output in Jupyter\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# Define colors for small and large models\n",
    "SMALL_MODEL_COLOR = 'blue'\n",
    "LARGE_MODEL_COLOR = 'red'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Add special tokens to the tokenizer if needed\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>\",\n",
    "        \"<|end_header_id|>\",\n",
    "        \"<|eot_id|>\",\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load both models\n",
    "model_small_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_large_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model_small = AutoModelForCausalLM.from_pretrained(\n",
    "    model_small_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model_large = AutoModelForCausalLM.from_pretrained(\n",
    "    model_large_id, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Resize token embeddings if special tokens were added\n",
    "model_small.resize_token_embeddings(len(tokenizer))\n",
    "model_large.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model_small.eval()\n",
    "model_large.eval()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_small.to(device)\n",
    "model_large.to(device)\n",
    "\n",
    "# Define the conversation as a string prompt\n",
    "def build_prompt(conversation):\n",
    "    \"\"\"\n",
    "    Build the prompt string from a list of conversation turns.\n",
    "    Each turn is a tuple: (role, content)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for role, content in conversation:\n",
    "        if role == \"system\":\n",
    "            prompt += (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "                f\"{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"user\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "            )\n",
    "        elif role == \"assistant\":\n",
    "            prompt += (\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    return prompt\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an intelligent AI assistant trained to do complex reasoning. You should carefully think about the question before outputting your final response.\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"How many 'r's are there in the word strawberry?\",\n",
    "    ),\n",
    "    (\"assistant\", \"\"),  # Start of assistant's reply\n",
    "]\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(conversation)\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "generated = input_ids\n",
    "max_new_tokens = 100  # Maximum number of new tokens to generate\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the EOS token ID\n",
    "eos_token = \"<|eot_id|>\"\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "# Prepare to store entropy and varentropy values\n",
    "entropies = []\n",
    "varentropies = []\n",
    "\n",
    "# Flags and variables to manage model switching\n",
    "current_model = model_small\n",
    "\n",
    "is_first_pass = True\n",
    "small_model_past_key_values = None\n",
    "large_model_past_key_values = None\n",
    "tokens_since_last_switch = 0  # Number of tokens generated since last model switch\n",
    "\n",
    "# For storing generated tokens and their colors\n",
    "generated_tokens = []\n",
    "generated_colors = []\n",
    "\n",
    "# Generate tokens one by one\n",
    "for _ in range(max_new_tokens):\n",
    "    if is_first_pass:\n",
    "        # For the first pass, generate tokens with the small model\n",
    "        small_outputs = model_small(input_ids=generated)\n",
    "        small_model_past_key_values = small_outputs.past_key_values\n",
    "        large_outputs = model_large(input_ids=generated)\n",
    "        large_model_past_key_values = large_outputs.past_key_values\n",
    "        is_first_pass = False\n",
    "        outputs = large_outputs\n",
    "    else:\n",
    "        # For subsequent passes, use the current model\n",
    "        current_past_key_values = small_model_past_key_values if current_model == model_small else large_model_past_key_values\n",
    "        outputs = current_model(\n",
    "            input_ids=generated[:, -1:], past_key_values=current_past_key_values\n",
    "        )\n",
    "        if current_model == model_small:\n",
    "            small_model_past_key_values = outputs.past_key_values\n",
    "        else:\n",
    "            large_model_past_key_values = outputs.past_key_values\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Scale the logits by the temperature parameter\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "\n",
    "    # Apply softmax to convert to probabilities\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute entropy and varentropy\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1) / math.log(2)  # Convert to bits\n",
    "    varentropy = torch.sum(\n",
    "        probs * ((log_probs / math.log(2) + entropy.unsqueeze(-1)) ** 2), dim=-1\n",
    "    )\n",
    "    entropies.append(entropy.item())\n",
    "    varentropies.append(varentropy.item())\n",
    "\n",
    "    # Adjust sampling strategy based on entropy and varentropy\n",
    "    if entropy.item() < 0.5 and varentropy.item() < 0.1:\n",
    "        # Low entropy and low varentropy: model is confident\n",
    "        # Use the smaller model if not already using it\n",
    "        if current_model != model_small:\n",
    "            print(\"\\nSwitching to small model (confident).\")\n",
    "            current_model = model_small\n",
    "            # Recompute past_key_values for the small model\n",
    "            # We need to process the generated tokens since last switch\n",
    "            # TODO make sure this logic is correct\n",
    "            generated_tokens_to_reprocess = generated[:, -tokens_since_last_switch:]\n",
    "            outputs = current_model(input_ids=generated_tokens_to_reprocess, past_key_values=small_model_past_key_values)\n",
    "            small_model_past_key_values = outputs.past_key_values\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Take argmax (greedy)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        sampling_info = \"Greedy sampling (confident)\"\n",
    "    elif entropy.item() > 3.0 and varentropy.item() > 1:\n",
    "        # High entropy and high varentropy: model is very uncertain\n",
    "        # Use the larger model if not already using it\n",
    "        if current_model != model_large:\n",
    "            print(\"\\nSwitching to large model (uncertain).\")\n",
    "            current_model = model_large\n",
    "            # Recompute past_key_values for the large model\n",
    "            # We need to process all generated tokens\n",
    "            # TODO make sure this logic is correct\n",
    "            generated_tokens_to_reprocess = generated[:, -tokens_since_last_switch:]\n",
    "            outputs = current_model(input_ids=generated_tokens_to_reprocess, past_key_values=large_model_past_key_values)\n",
    "            large_model_past_key_values = outputs.past_key_values\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            tokens_since_last_switch = 0\n",
    "        # Continue sampling with adjusted parameters\n",
    "        adjusted_temperature = min(1.5, temperature * 1.3)\n",
    "        next_token_logits = next_token_logits / adjusted_temperature\n",
    "        # Sample the next token\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = f\"Sampling with increased temperature {adjusted_temperature:.2f} (uncertain)\"\n",
    "    else:\n",
    "        # Default sampling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        sampling_info = \"Default sampling\"\n",
    "\n",
    "    # Append the next token to the generated sequence\n",
    "    generated = torch.cat((generated, next_token), dim=-1)\n",
    "    tokens_since_last_switch += 1\n",
    "\n",
    "    # Decode the newly generated token\n",
    "    decoded_token = tokenizer.decode(next_token[0], skip_special_tokens=False)\n",
    "\n",
    "    # Store the token and its associated color\n",
    "    generated_tokens.append(decoded_token)\n",
    "    if current_model == model_small:\n",
    "        generated_colors.append(SMALL_MODEL_COLOR)\n",
    "    else:\n",
    "        generated_colors.append(LARGE_MODEL_COLOR)\n",
    "\n",
    "    # Display the generated text with colors\n",
    "    # Build the HTML content\n",
    "    html_content = ''\n",
    "    for token, color in zip(generated_tokens, generated_colors):\n",
    "        html_content += f'<span style=\"color: {color}\">{token}</span>'\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Stop if the EOS token is generated\n",
    "    if next_token.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "# After generation, print the final generated text\n",
    "print(\"\\nGenerated Text:\")\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
